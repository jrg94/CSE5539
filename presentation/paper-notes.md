# Presentation Notes

On 4/15, I'll be presenting the following chapter and paper in class:

Goto M. (2006): “Analysis of musical audio signals,” In Wang D.L. & Brown G.J. (eds.):
Computational auditory scene analysis: Principles, algorithms, and applications. IEEE
Press/Wiley, Hoboken NJ, Chapter 8, pp. 251-295.

Rafii Z. and Pardo B. (2013): “REpeating Pattern Extraction Technique (REPET): A simple
method for music/voice separation,” IEEE/ACM Transactions on Audio, Speech, and
Language Processing, vol. 21, pp. 71-82.

Naturally, I'll be using this space to record my notes.

## [Analysis of Musical Audio Signals][1]

This chapter is broken up into sections, so I'll summarize them separately.

### 8.1: Introduction (251 - 252)

The introduction covers the idea that people are capable of comprehending music
without necessarily separate all the individuals sounds. In other words, they
understand the mixture of sounds. Obviously, we'd like to be able to replicate
that behavior in a computer.

### 8.2: Music Scene Description (252 - 256)

The Music Scene Description section introduces the capabilities of untrained music
listeners (i.e. hum melody, clapping on beat, etc.). As a result, a system for
describing music should not consider technical musical details like individual
notes in a chord but rather its overall tone color.

#### 8.2.1 Music Scene Descriptions (253 - 255)

The Music Scene Descriptions section introduces three concepts: 

- Local Frequency Structure (Melody Lines vs. Base Lines)
- Local Temporal Structure (Measure Levels)
- Global Music Structure (Repeated/Chorus Sections)

In general, this section covers the history of music scene analysis.
For example, music scene analysis has picked up since the 1990s due
to processing speeds allowing for processor intensive calculations like
fast fourier transforms and other statistical methods. Statistical models
include the Hidden Markov Model and Optima.

#### 8.2.2 Difficulties Associated with Musical Audio Signals (255 - 256)

Musical audio signals are complicated and can depend on different factors
such as monaural or stereo (channels), monophonic or polyphonic (number of
sounds), and genre. Granted, stereo signals can be easily reduced to
monaural signals by averaging the channels. That said, generally music signals
are polyphonic, and genre matters because beat structure is more
pronounced in some genres (i.e. pop) than others.

### 8.3 Estimating Melody and Base Lines (256 - 266)

This sections opens up with some application domains for estimating melody
and base lines such as retreiving a musical piece by humming or automatic
production of accompaniment tracks for karaoke.

Later, the section discusses challenges with using current techniques like
F0 estimation which depends on single tones that lack periodic noise. Even
special F0 estimation methods fail due to the smearing of harmonics.

Fortunately, F0 estimation in music was first achieved in 1999 using PreFEst.
PreFEst works by estimating F0 from the most predominant harmonic structure
using a probability density function (PDF). Unfortunately, I struggled to
grasp exactly how it works from all the signal processing jargon. Luckily,
this section claims that the process will be explained in more detail. That
said, the basic model works in three parts:

1. Front end: frequency analysis
2. Core: predominant F0 estimation
3. Back end: temporal continuity evaluation

Based on this model, melody and bass lines can be extracted using frequency-
range limitations during core processing.

#### 8.3.1 PreFEst-front-end: Forming the Observed Probability Density Function (258)

The PDF is generated by passing the music signal through a multirate filter
bank and extracting two frequency components via two band pass filters: 
261.6-4186 Hz for melody and 32.7-261.6 Hz for bass line. From there,
the sets of components are represented as an observed PDF. 

#### 8.3.2 PreFEst-core: Estimating the F0's Probability Density Function (258)

Using the observed PDFs, the core forms a PDF of the F0. I'll need to reread this
section to really get it.

##### 8.3.2.1 Weighted-Mixture Model of Adaptive Tone Models (258 - 260)

An observed PDF is assumed to be generated from a model which is a weighted
mixture of all possible tone models (whatever that means). 

##### 8.3.2.2 Introducing a Prior Distribution

How to create a prior distribution?

##### 8.3.2.3 MAP Estimation Using the EM Algorithm

Apparently, it's too hard to perform an integral, so we use
some Expectation-Maximization algorithm to compute MAP
estimates from incomplete observed data. 

Apparently, this complex mathematical solution is better than
comb-filter based and autocorrelation-based multiple F0
estimation methods because they cannot separate overlapping
frequency components. 

#### 8.3.3 PreFEst-back-end: Sequential F0 Tracking by Multiple-Agent Architecture

F0 is tracked over time. The most predominant and stable F0 is then selected. 

#### 8.3.4 Other Methods

PreFEst is great, but it has drawbacks--namely, bias can be including in detection.

Paiva, Mendes, and Cardoso implemented a solution based on human anatomy that can
generate MIDI output rather than an F0 trajectory. The solution leverages correlograms
and forms temporal trajectories of F0 candidates. These candidates are quantized to
the closest MIDI note numbers and some analysis is done to remove transient notes
and harmonics. Finally, the best sequence is chosen based on some heuristic. 

Meanwhile, Marolt leverages PreFEst core to estimate F0 candidates, but uses
spectral modeling synthese (SMS) on the front end. The advantage to this method
is the ability to identify melody fragments which can be clustered into a 
melody line. Clustering is accomplished using Gaussian Mixture Models (GMMs)
based on dominance, pitch, loudness, pitch stability, and onset steepness. 

For classical music, Eggink and Brown developed a method of detecting
melody lines using various knowledge sources for prediction. Knowledge
sources include local knowledge (i.e. instrument recognition) and temporal 
knowledge. I'll need to read this again to get a good understanding.

For vocal music, Li and Wang extend F0 estimation from noisy speech techniques.
Music signals are sent through a 128 gammatone filterbank and split by a
frequency of 800 Hz. Then some analysis is done to extract the melody line.

Finally, Hainsworth and Macleod came up with a method for detecting base lines.
First, they extract onset times of notes below 200 Hz. Then, they perform some 
F0 analysis and track the results over time using comb-filter like analysis.
The results are then cleaned up.

### 8.4 Estimating Beat Structure (267 - 275)

Once again, this section kicks off with some applications of estimating beat
structure such as music-synchronized computer graphics, stage-lighting control, 
and human-computer improvisation in live ensembles.

Next, the section lists a ton of historical work that went into tracking beat
in MIDI and CD recordings over the years. 

After that, the section defines what hierarchical beat structure is: quarter note
and bar line tracking. Quarter notes are tracked through period and phase
and measures are track through sequences of quarter notes.

Finally, they introduce the challenges of tracking beats in CD musics. Namely:

1. Estimating the period and phase by using cues in audio singlas
  - Current techniques fail in in polyphonic audio signals
2. Dealing with ambiguity of interpretation
  - Multiple interpretations of beats are possibles
3. Using musical knowledge to make musical decisions
  - Musical knowledge allows for reduction of ambiguity

### 8.5 Estimating Chorus Sections and Repeated Sections (275 - 286)

### 8.6 Discussions and Conclusions (286 - 289)

## [REpeating Pattern Extraction Technique (REPET)][2]

At a high level, REPET is a tool for extracting repeating pattern audio from
a musical track. It can be used to separate vocals from a musical track.

[1]: #
[2]: http://music.cs.northwestern.edu/publications/Rafii-Pardo%20-%20REpeating%20Pattern%20Extraction%20Technique%20(REPET)%20A%20Simple%20Method%20for%20Music-Voice%20Separation%20-%20TALSP%202013.pdf
