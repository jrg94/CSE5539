# Presentation Notes

On 4/15, I'll be presenting the following chapter and paper in class:

Goto M. (2006): “Analysis of musical audio signals,” In Wang D.L. & Brown G.J. (eds.):
Computational auditory scene analysis: Principles, algorithms, and applications. IEEE
Press/Wiley, Hoboken NJ, Chapter 8, pp. 251-295.

Rafii Z. and Pardo B. (2013): “REpeating Pattern Extraction Technique (REPET): A simple
method for music/voice separation,” IEEE/ACM Transactions on Audio, Speech, and
Language Processing, vol. 21, pp. 71-82.

Naturally, I'll be using this space to record my notes.

## [Analysis of Musical Audio Signals][1]

This chapter is broken up into sections, so I'll summarize them separately.

### 8.1: Introduction (251 - 252)

The introduction covers the idea that people are capable of comprehending music
without necessarily separate all the individuals sounds. In other words, they
understand the mixture of sounds. Obviously, we'd like to be able to replicate
that behavior in a computer.

### 8.2: Music Scene Description (252 - 256)

The Music Scene Description section introduces the capabilities of untrained music
listeners (i.e. hum melody, clapping on beat, etc.). As a result, a system for
describing music should not consider technical musical details like individual
notes in a chord but rather its overall tone color.

#### 8.2.1 Music Scene Descriptions (253 - 255)

The Music Scene Descriptions section introduces three concepts: 

- Local Frequency Structure (Melody Lines vs. Base Lines)
- Local Temporal Structure (Measure Levels)
- Global Music Structure (Repeated/Chorus Sections)

In general, this section covers the history of music scene analysis.
For example, music scene analysis has picked up since the 1990s due
to processing speeds allowing for processor intensive calculations like
fast fourier transforms and other statistical methods. Statistical models
include the Hidden Markov Model and Optima.

#### 8.2.2 Difficulties Associated with Musical Audio Signals (255 - 256)

Musical audio signals are complicated and can depend on different factors
such as monaural or stereo (channels), monophonic or polyphonic (number of
sounds), and genre. Granted, stereo signals can be easily reduced to
monaural signals by averaging the channels. That said, generally music signals
are polyphonic, and genre matters because beat structure is more
pronounced in some genres (i.e. pop) than others.

### 8.3 Estimating Melody and Base Lines (256 - 266)

This sections opens up with some application domains for estimating melody
and base lines such as retreiving a musical piece by humming or automatic
production of accompaniment tracks for karaoke.

Later, the section discusses challenges with using current techniques like
F0 estimation which depends on single tones that lack periodic noise. Even
special F0 estimation methods fail due to the smearing of harmonics.

Fortunately, F0 estimation in music was first achieved in 1999 using PreFEst.
PreFEst works by estimating F0 from the most predominant harmonic structure
using a probability density function (PDF). Unfortunately, I struggled to
grasp exactly how it works from all the signal processing jargon. Luckily,
this section claims that the process will be explained in more detail. That
said, the basic model works in three parts:

1. Front end: frequency analysis
2. Core: predominant F0 estimation
3. Back end: temporal continuity evaluation

Based on this model, melody and bass lines can be extracted using frequency-
range limitations during core processing.

#### 8.3.1 PreFEst-front-end: Forming the Observed Probability Density Function (258)

The PDF is generated by passing the music signal through a multirate filter
bank and extracting two frequency components via two band pass filters: 
261.6-4186 Hz for melody and 32.7-261.6 Hz for bass line. From there,
the sets of components are represented as an observed PDF. 

#### 8.3.2 PreFEst-core: Estimating the F0's Probability Density Function (258)

Using the observed PDFs, the core forms a PDF of the F0. I'll need to reread this
section to really get it.

##### 8.3.2.1 Weighted-Mixture Model of Adaptive Tone Models (258 - 260)

### 8.4 Estimating Beat Structure (267 - 275)

### 8.5 Estimating Chorus Sections and Repeated Sections (275 - 286)

### 8.6 Discussions and Conclusions (286 - 289)

## [REpeating Pattern Extraction Technique (REPET)][2]

At a high level, REPET is a tool for extracting repeating pattern audio from
a musical track. It can be used to separate vocals from a musical track.

[1]: #
[2]: http://music.cs.northwestern.edu/publications/Rafii-Pardo%20-%20REpeating%20Pattern%20Extraction%20Technique%20(REPET)%20A%20Simple%20Method%20for%20Music-Voice%20Separation%20-%20TALSP%202013.pdf
